# ConvIsing
This repository contains an API for learning the coarse-grained Hamiltonian on the simplest nearest-neighbor classical Ising model, and using repeated renormalizations to estimate the critical exponent. It requires Ising samples to have already been generated (some example Julia code included). Examples of how to use the API are in `learn.py`.

[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Features
Current feature list:
- Models:
  - Convolutional networks of the form [convolutional layer] -> [dense, fully connected layers] -> [sum layer] -> linear combination layer
  - Linear combinations of a fixed nearest neighbor, next nearest neighbor, and four spin basis
- Ising options:
  - lattice size L
  - inverse temperature beta
  - coarse-graining method
  - coarse-graining factor
- Training options:
  - Choose how much data to keep or throw away
  - Choose the training/validation/testing split
  - Choose training/architecture parameters:
    - batch size
    - number of epochs
    - activation functions
    - kernel size of the convolution
    - number of convolutional filters
  - If working with 4x4 lattices, can exactly compute the coarse-grained Hamiltonian
  - Can freeze all but the linear combination layer
- Generate coarse-grained data
- Computes a number of metrics:
  - Records the loss function and saves losses and various metrics to a log
  - Can estimate the critical exponent at different coarse-graining levels
  - Can run several Metropolis chains in parallel to compare observable statistics between samples generated from the learned model and the data

Todo:
- It would be nice if the Julia code included coarse-grained data generation
- Might want to be able to save the samples generated from the learned models

## Basic API reference
There are two main things that can be done: creating and training a network, and generating samples.

Creating and training a network:
- Create Ising configuration and Training configuration dictionaries
- Create the coarse-grained data if necessary: `data.create_cg_dataset`
- Load the coarse-grained data: `data.load_dataset`
- Create a pair of Keras models, one for energy and one for exponentiated energy differences: `model_group = models.ModelGroup`
- Train the model on the coarse-grained data (and save metrics): `trn.train_and_save`

Generating samples and/or computing relevant observables:
- Create an instance of the observables/MCMC class: `obs = mcmc.Observables(f, cgL, num_samples, num_chains, num_burn, batch_size, skip)`
  - `f` is a function which evaluates an exponentiated energy difference, e.g. `f = model_group.ediff.predict`
  - Lattices of size `cgL`x`cgL` will be created (usually want this to be smaller than the `L` in the configuration class)
  - `num_samples` is the number of samples that will be generated, or that you will feed into the class
  - `num_chains` is the number of parallel Metropolis chains to be run if using the metropolis method
  - `num_burn` is the number of samples to discard in each chain before computing the observables
  - Observables are computed by looping over the data in batches (especially useful in the case when the data is too large to be loaded into memory), and `batch_size` controls the number of samples in a batch
  - `skip` is the number of samples to discard in each chain between updates to the observables (to reduce correlation between the samples)
- If you already have data of the form `N`x`cgL`x`cgL` or `N`x`cgL`^2, you can run `obs.compute_observables(dataset, batch_size)` to compute the observables, and you should set `obs.num_recorded = N`
- If you want to generate samples using the exponentiated energy difference `f`, you can run `obs.metrop_par(batch_size)` which will automatically update the observables as it is run
- Print the computed observables: `obs.print_observables()`

See `learn.py` for examples of usage. Samples (not coarse-grained) can be generated by modifying and running the included Julia files (`gen_critical_temp.jl`).
