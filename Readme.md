# ConvIsing
This is a package for learning the coarse-grained Hamiltonian on the simplest nearest-neighbor classical Ising model, and using repeated renormalizations to estimate the critical exponent. It requires Ising samples to have already been generated (some example Julia code included).

## Features
Current feature list:
- Models:
  - Convolutional networks of the form [convolutional layer] -> [dense, fully connected layers] -> [sum layer] -> linear combination layer
  - Linear combinations of a fixed nearest neighbor, next nearest neighbor, and four spin basis
- Training options:
  - Choose how much data to keep or throw away
  - Choose the training/validation/testing split
  - Choose coarse-graining method and factor
  - Choose training parameters:
    - batch size
    - number of epochs
    - number of gpus
  - If working with 4x4 lattices, can exactly compute the coarse-grained Hamiltonian
  - Can freeze all but the linear combination layer
- Generate coarse-grained data
- Uses a generator class to feed data on the fly to a Keras model
- Computes a number of metrics:
  - Records the loss function
  - Can estimate the critical exponent
  - Can run several Metropolis chains in parallel to compare observable statistics between samples generated from the learned model and the data

Todo:
- It would be nice if the Julia code included coarse-grained data generation
- Might want to be able to save the samples generated from the learned models
- An eventual refactoring of the code may be in order

## Basic API reference
There are two main things that can be done: creating and training a network, and generating samples.
Creating and training a network:
- Create an instance of the configuration class, `config = train.Config(L, beta, cg_method, cg_factor)`, and adjust other training parameters. See the documentation block for `train.Config`
- Create an instance of the training class, `trn = train.ConvIsing(config)`
  - Create the coarse-grained data if necessary: `trn.create_cg_dataset(config)`
  - Load the coarse-grained data: `trn.load_dataset(config)`
  - Train the model on the coarse-grained data: `trn.run_model(config)`
  - Reload the weights if necessary: `trn.reload_weights(config)`
  - Compute and print some metrics:
    - Compute the final loss and estimate the critical exponent: `trn.compute_metrics(config)`
    - Print the computed metrics: `trn.print_metrics()`
    - Graph the epochs vs. the loss function (both training and validation loss): `trn.graph_loss(config)`
Generating samples and/or computing relevant observables:
- Create an instance of the observables/MCMC class: `obs = mcmc.Observables(f, cgL, num_samples, num_chains, num_burn, batch_size, skip)`
  - `f` is a function which evaluates an exponentiated energy difference, e.g. `f = trn.model.predict`
  - Lattices of size `cgL`x`cgL` will be created (usually want this to be smaller than the `L` in the configuration class)
  - `num_samples` is the number of samples that will be generated, or that you will feed into the class
  - `num_chains` is the number of parallel Metropolis chains to be run if using the metropolis method
  - `num_burn` is the number of samples to discard in each chain before computing the observables
  - Observables are computed by looping over the data in batches (especially useful in the case when the data is too large to be loaded into memory), and `batch_size` controls the number of samples in a batch
  - `skip` is the number of samples to discard in each chain between updates to the observables (to reduce correlation between the samples)
- If you already have data of the form `N`x`cgL`x`cgL` or `N`x`cgL`^2, you can run `obs.compute_observables(dataset, batch_size)` to compute the observables, and you should set `obs.num_recorded = N`
- If you want to generate samples using the exponentiated energy difference `f`, you can run `obs.metrop_par(batch_size)` which will automatically update the observables as it is run
- Print the computed observables: `obs.print_observables()`

See `learn.py` for some basic examples of usage. Samples (not coarse-grained) can be generated by modifying and running the included Julia files (`gen_critical_temp.jl`).
